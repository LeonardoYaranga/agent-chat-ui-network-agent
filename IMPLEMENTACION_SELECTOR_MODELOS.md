# Cambios Implementados: Selector Din√°mico de Modelos LLM

## üìã Resumen

Se ha implementado la funcionalidad para **cambiar din√°micamente el proveedor y modelo LLM** desde el frontend **sin necesidad de reiniciar el servidor de LangGraph**.

## ‚ú® Caracter√≠sticas Implementadas

### 1. **Backend - Runtime Configuration**

- ‚úÖ Modificado `llm-provider.ts` para aceptar configuraci√≥n din√°mica (`LLMConfig`)
- ‚úÖ Actualizado `agent.ts` para leer `config?.configurable?.llmConfig` en cada request
- ‚úÖ Creado `models.ts` con cat√°logo de modelos disponibles por proveedor

### 2. **Frontend - Selector de Modelos**

- ‚úÖ Componente `ModelSelector` con UI elegante para cambiar modelos
- ‚úÖ Contexto `ModelContext` para gestionar el modelo seleccionado globalmente
- ‚úÖ Integraci√≥n con `StreamProvider` para pasar config en cada request
- ‚úÖ Persistencia del modelo seleccionado en `localStorage`
- ‚úÖ Notificaciones visuales al cambiar de modelo

### 3. **Modelos Disponibles**

#### OpenRouter (Gratis)

- GPT-4o Mini ‚ö° (predeterminado)
- GPT OSS 120B üß† (razonamiento complejo)
- Devstral 2512 ü§ñ (especialista en agentes)
- Nemotron 3 Nano 30B üî¨ (razonamiento)
- KAT Coder Pro üíª (especialista en c√≥digo)

#### Google Gemini

- Gemini 2.0 Flash Experimental ‚ö°
- Gemini 2.5 Flash
- Gemini 1.5 Pro üß†

#### LM Studio (Local)

- Qwen 3 4B (con thinking) üí≠
- Qwen 3 1.7B (con thinking) üí≠

## üéØ C√≥mo Funciona

### Arquitectura

```
Frontend (ModelSelector)
    ‚Üì selecciona modelo
ModelContext (llmConfig)
    ‚Üì pasa en config
StreamProvider (useStream)
    ‚Üì incluye en request
LangGraph Server (:2024)
    ‚Üì lee de config.configurable
agent.ts (callModel)
    ‚Üì usa llmConfig
llm-provider.ts (getLLMProvider)
    ‚Üì crea modelo espec√≠fico
Modelo LLM seleccionado ‚ú®
```

### Flujo de Datos

1. **Usuario selecciona modelo** en el UI (bot√≥n con √≠cono ‚ú®)
2. **ModelContext actualiza** `llmConfig` con `{ provider, model }`
3. **StreamProvider pasa** config en cada `submit()`/`stream()`:
   ```typescript
   config: {
     configurable: {
       llmConfig: { provider: "gemini", model: "gemini-2.0-flash-exp" }
     }
   }
   ```
4. **Backend lee** `config?.configurable?.llmConfig`
5. **getLLMProvider** crea el LLM espec√≠fico din√°micamente
6. **Conversaci√≥n contin√∫a** con el nuevo modelo (sin reinicio)

## üöÄ Uso

### En el Frontend

El selector aparece en el header de la aplicaci√≥n:

```tsx
<ModelSelector
  selectedModel={selectedModel}
  onModelChange={(config) => {
    setModel(config, model);
    toast.success(`Modelo cambiado a ${model.name}`);
  }}
/>
```

### Agregar Nuevos Modelos

Edita `src/lib/models.ts` (frontend) y `src/config/models.ts` (backend):

```typescript
export const OPENROUTER_MODELS: ModelInfo[] = [
  {
    id: "mi-nuevo-modelo",
    name: "Mi Nuevo Modelo",
    provider: "openrouter",
    model: "company/model-name",
    description: "Descripci√≥n del modelo",
    capabilities: {
      streaming: true,
      tools: true,
      reasoning: true,
    },
  },
  // ... otros modelos
];
```

## üì¶ Archivos Modificados/Creados

### Backend (`mcp_client_langchain_network_agent/`)

- ‚úèÔ∏è `src/config/llm-provider.ts` - Acepta `LLMConfig` opcional
- ‚úèÔ∏è `src/graph/agent.ts` - Lee `llmConfig` de runtime config
- ‚ú® `src/config/models.ts` - Cat√°logo de modelos disponibles

### Frontend (`agent-chat-ui-network-agent/`)

- ‚ú® `src/lib/models.ts` - Definiciones de modelos y proveedores
- ‚ú® `src/components/ModelSelector.tsx` - Componente selector de modelos
- ‚ú® `src/components/ui/popover.tsx` - Componente Popover (Radix UI)
- ‚ú® `src/components/ui/command.tsx` - Componente Command (cmdk)
- ‚ú® `src/contexts/ModelContext.tsx` - Contexto global del modelo
- ‚úèÔ∏è `src/providers/Stream.tsx` - Pasa `llmConfig` en requests
- ‚úèÔ∏è `src/app/layout.tsx` - Envuelve con `ModelProvider`
- ‚úèÔ∏è `src/components/thread/index.tsx` - Integra `ModelSelector` en UI

## üîß Dependencias Instaladas

```bash
pnpm add @radix-ui/react-popover cmdk
```

## ‚ö†Ô∏è Notas Importantes

### 1. **NO se requiere reiniciar el servidor**

La configuraci√≥n se pasa por request mediante `config.configurable`, lo que permite cambiar modelos en caliente.

### 2. **Compatibilidad con LangGraph**

Utiliza la funcionalidad nativa de LangGraph de **runtime configuration**, documentada en:

- [LangGraph Runtime Configuration](https://docs.langchain.com/langsmith/configurable-headers)
- [Dynamic Model Selection](https://docs.langchain.com/oss/python/migrate/langchain-v1)

### 3. **Persistencia**

El modelo seleccionado se guarda en `localStorage` para mantenerlo entre recargas.

### 4. **Variables de Entorno**

Aseg√∫rate de tener configuradas las API keys en `.env`:

```bash
OPENROUTER_API_KEY=sk-or-v1-...
GEMINI_API_KEY=AIzaSy...
LMSTUDIO_BASE_URL=http://localhost:1234/v1
```

### 5. **Modelo Predeterminado**

Si no se selecciona ninguno, usa `openai/gpt-4o-mini` (configurado en `DEFAULT_MODEL`)

## üé® Capacidades del Selector

- **B√∫squeda r√°pida** de modelos
- **Agrupaci√≥n por proveedor**
- **Badges visuales** para capacidades (thinking, reasoning, coding)
- **Cambio autom√°tico** al seleccionar proveedor
- **Informaci√≥n detallada** de cada modelo (nombre t√©cnico, descripci√≥n)

## ‚úÖ Testing

Para probar:

1. Inicia el backend:

   ```bash
   cd mcp_client_langchain_network_agent
   pnpm dev
   ```

2. Inicia el frontend:

   ```bash
   cd agent-chat-ui-network-agent
   pnpm dev
   ```

3. Abre http://localhost:3000
4. Haz clic en el bot√≥n del modelo (con √≠cono ‚ú®) en el header
5. Selecciona un proveedor y luego un modelo
6. ¬°Observa c√≥mo cambia inmediatamente!

## üéØ Pr√≥ximas Mejoras Posibles

- [ ] Mostrar estad√≠sticas del modelo (tokens, costo)
- [ ] Permitir ajustar `temperature` desde la UI
- [ ] Historial de modelos usados
- [ ] Comparar respuestas entre modelos
- [ ] Auto-selecci√≥n basada en el tipo de tarea

---

**Implementado por**: GitHub Copilot
**Fecha**: 2026-01-01
**Versi√≥n**: 1.0.0
