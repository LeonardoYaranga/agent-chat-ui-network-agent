# üîÑ Sincronizaci√≥n de Modelos entre Frontend y Backend

## ‚ùì ¬øPor qu√© existen dos archivos models.ts?

Tienes dos archivos que definen los modelos:

- üìÅ `mcp_client_langchain_network_agent/src/config/models.ts` (Backend)
- üìÅ `agent-chat-ui-network-agent/src/lib/models.ts` (Frontend)

**¬øDeber√≠an ser iguales?** ‚Üí **S√ç, la lista de modelos debe coincidir**

---

## üéØ ¬øC√≥mo funciona el flujo completo?

### Paso a Paso:

#### 1Ô∏è‚É£ **Frontend muestra modelos disponibles**

```typescript
// Frontend: src/lib/models.ts
export const GEMINI_MODELS: ModelInfo[] = [
  {
    id: "gemini-2.5-flash",
    name: "Gemini 2.5 Flash",
    model: "gemini-2.5-flash", // ‚Üê Este es el ID del modelo
    //...
  },
];
```

El `ModelSelector` lee esta lista y la muestra en el UI.

#### 2Ô∏è‚É£ **Usuario selecciona un modelo**

```tsx
// Usuario hace click en "Gemini 2.5 Flash"
handleModelSelect(model);
```

#### 3Ô∏è‚É£ **Frontend env√≠a configuraci√≥n al backend**

```typescript
// Frontend: thread/index.tsx
stream.submit(
  { messages },
  {
    config: {
      configurable: {
        llmConfig: {
          provider: "gemini", // ‚Üê Proveedor
          model: "gemini-2.5-flash", // ‚Üê ID del modelo
        },
      },
    },
  }
);
```

**Esto va al puerto 2024 (LangGraph Server)** mediante el SDK de LangGraph.

#### 4Ô∏è‚É£ **Backend recibe la configuraci√≥n**

```typescript
// Backend: graph/agent.ts
async function callModel(state: typeof AgentState.State, config) {
  const llmConfig = config?.configurable?.llmConfig;
  // llmConfig = { provider: "gemini", model: "gemini-2.5-flash" }

  const llm = getLLMProvider(llmConfig); // ‚Üê Crea el LLM
}
```

#### 5Ô∏è‚É£ **Backend busca el modelo en su lista**

```typescript
// Backend: config/llm-provider.ts
export const getLLMProvider = (config?: LLMConfig): BaseChatModel => {
  const provider = config?.provider || process.env.LLM_PROVIDER;
  const modelId = config?.model || process.env.LLM_MODEL;

  if (provider === "gemini") {
    return new ChatGoogleGenerativeAI({
      apiKey: process.env.GEMINI_API_KEY,
      model: modelId, // ‚Üê "gemini-2.5-flash"
      // Busca configuraci√≥n espec√≠fica en models.ts
    });
  }
};
```

#### 6Ô∏è‚É£ **Backend usa configuraci√≥n del modelo**

```typescript
// Backend: config/models.ts
export const GEMINI_MODELS: ModelInfo[] = [
  {
    model: "gemini-2.5-flash",
    config: {
      maxTokens: 8000, // ‚Üê Configuraci√≥n espec√≠fica
      timeout: 300000,
    },
  },
];
```

El backend necesita saber:

- ‚úÖ Qu√© modelos son v√°lidos
- ‚úÖ Configuraci√≥n espec√≠fica de cada modelo (tokens, timeout)
- ‚úÖ Validar que el frontend no envi√≥ un modelo inv√°lido

---

## üîç ¬øPor qu√© el backend tiene todos los modelos?

### Razones:

#### 1. **Validaci√≥n**

Si el frontend env√≠a `model: "modelo-que-no-existe"`, el backend puede:

- Detectarlo
- Usar el modelo por defecto
- Registrar un warning

#### 2. **Configuraci√≥n Espec√≠fica**

Cada modelo tiene configuraci√≥n diferente:

```typescript
{
  "gemini-2.5-flash": {
    maxTokens: 8000,
    timeout: 300000
  },
  "gemini-robotics-er-1.5-preview": {
    maxTokens: 8000,
    timeout: 300000
  }
}
```

#### 3. **Fallback/Default**

Si el frontend NO env√≠a configuraci√≥n:

```typescript
// Backend usa DEFAULT_MODEL
export const DEFAULT_MODEL = OPENROUTER_MODELS[0]; // GPT-4o Mini
```

#### 4. **Metadata del Modelo**

El backend puede necesitar saber:

- ¬øSoporta streaming?
- ¬øSoporta function calling (tools)?
- ¬øTiene capacidades especiales (thinking, reasoning)?

---

## üìä Comparaci√≥n: ¬øQu√© va en cada archivo?

| Caracter√≠stica                  | Frontend | Backend     | ¬øDebe coincidir? |
| ------------------------------- | -------- | ----------- | ---------------- |
| **Lista de modelos**            | ‚úÖ S√≠    | ‚úÖ S√≠       | ‚úÖ **S√ç**        |
| **ID del modelo**               | ‚úÖ S√≠    | ‚úÖ S√≠       | ‚úÖ **S√ç**        |
| **Nombre para mostrar**         | ‚úÖ S√≠    | ‚ö†Ô∏è Opcional | ‚ùå No cr√≠tico    |
| **Descripci√≥n**                 | ‚úÖ S√≠    | ‚ö†Ô∏è Opcional | ‚ùå No cr√≠tico    |
| **Capabilities (UI badges)**    | ‚úÖ S√≠    | ‚ö†Ô∏è Opcional | ‚ùå No cr√≠tico    |
| **Config (maxTokens, timeout)** | ‚ùå No    | ‚úÖ **S√≠**   | N/A              |
| **Providers list**              | ‚úÖ S√≠    | ‚ùå No       | N/A              |

### Lo CR√çTICO que debe coincidir:

```typescript
// ‚úÖ DEBE SER IGUAL EN AMBOS
{
  id: "gemini-2.5-flash",           // ‚Üê ID √∫nico
  provider: "gemini",                // ‚Üê Proveedor
  model: "gemini-2.5-flash"          // ‚Üê ID del modelo en la API
}
```

### Lo que puede ser diferente:

```typescript
// Frontend: Solo para UI
name: "Gemini 2.5 Flash ‚ö°"
description: "S√∫per r√°pido y econ√≥mico"

// Backend: Solo para configuraci√≥n t√©cnica
config: {
  maxTokens: 8000,
  timeout: 300000
}
```

---

## üö´ ¬øPor qu√© NO un endpoint API REST?

### Opci√≥n 1 (NO viable): Crear endpoint `/api/models`

```typescript
// ‚ùå NO HACER ESTO
app.get("/api/models", (req, res) => {
  res.json(ALL_MODELS);
});
```

**Problemas**:

- ‚ùå LangGraph Server (puerto 2024) NO es Express
- ‚ùå No puedes agregar rutas custom f√°cilmente
- ‚ùå Requerir√≠a un servidor Express adicional
- ‚ùå Sobrecomplica la arquitectura

### Opci√≥n 2 (La actual - ‚úÖ CORRECTA): Duplicar la lista

```typescript
// Frontend: models.ts
export const GEMINI_MODELS = [...]

// Backend: models.ts
export const GEMINI_MODELS = [...]
```

**Ventajas**:

- ‚úÖ Simple
- ‚úÖ No requiere llamadas API extra
- ‚úÖ El frontend carga instant√°neamente
- ‚úÖ Funciona offline (durante desarrollo)

**Desventaja**:

- ‚ö†Ô∏è Hay que mantener ambos archivos sincronizados manualmente

---

## üîÑ ¬øC√≥mo mantener sincronizados los archivos?

### Opci√≥n 1: Manual (actual)

Cuando agregues/modifiques un modelo:

1. Edita `backend/src/config/models.ts`
2. Edita `frontend/src/lib/models.ts`
3. Aseg√∫rate que `id`, `provider` y `model` coincidan

### Opci√≥n 2: Script de sincronizaci√≥n (futuro)

```bash
# En el futuro podr√≠as crear un script
pnpm run sync-models
```

Que copie la lista del backend al frontend.

### Opci√≥n 3: Workspace compartido (TypeScript)

Crear un paquete `@shared/models` que ambos importen:

```
packages/
  shared/
    models.ts        ‚Üê √önica fuente de verdad
  backend/
    imports from @shared/models
  frontend/
    imports from @shared/models
```

---

## ‚úÖ Estado Actual (Actualizado)

### Modelos Gemini sincronizados:

```typescript
// ‚úÖ BACKEND: mcp_client_langchain_network_agent/src/config/models.ts
// ‚úÖ FRONTEND: agent-chat-ui-network-agent/src/lib/models.ts

export const GEMINI_MODELS: ModelInfo[] = [
  {
    id: "gemini-robotics-er-1.5-preview",
    name: "Gemini Robotics ER 1.5 Preview",
    provider: "gemini",
    model: "gemini-robotics-er-1.5-preview",
    // ...
  },
  {
    id: "gemini-2.5-flash",
    name: "Gemini 2.5 Flash",
    provider: "gemini",
    model: "gemini-2.5-flash",
    // ...
  },
  {
    id: "gemini-2.5-flash-lite",
    name: "Gemini 2.5 Flash Lite",
    provider: "gemini",
    model: "gemini-2.5-flash-lite",
    // ...
  },
];
```

**Ahora ambos archivos est√°n sincronizados** ‚úÖ

---

## üéì Resumen para recordar

1. **Frontend models.ts**:

   - Lista modelos para mostrar en el UI
   - Usuario los ve y selecciona

2. **Backend models.ts**:

   - Valida que el modelo recibido es v√°lido
   - Obtiene configuraci√≥n espec√≠fica (tokens, timeout)
   - Proporciona modelo por defecto si no hay config

3. **Flujo**:

   ```
   Usuario selecciona ‚Üí Frontend env√≠a config ‚Üí Backend valida y usa
   ```

4. **Sincronizaci√≥n**:

   - ‚úÖ `id`, `provider`, `model` DEBEN coincidir
   - ‚ö†Ô∏è Mantener manualmente (por ahora)
   - üí° Futuro: script de sync o paquete compartido

5. **NO endpoint API**:
   - ‚ùå LangGraph no es Express
   - ‚úÖ LangGraph S√ç acepta `config.configurable`
   - ‚úÖ Ese es el patr√≥n correcto

---

**Actualizado**: 2026-01-02  
**Estado**: Frontend y Backend sincronizados ‚úÖ
